# AI Live Chat Support Agent

This project is a mini AI-powered customer support chat application.  
It simulates a live chat widget where users can ask questions (e.g., shipping, returns, support hours) and receive AI-generated responses via a real LLM API.

The goal of this project is to demonstrate **clean architecture, robustness, and realistic product behavior**.

---

## âœ¨ Features

- Live chat UI with user and AI messages
- Session-based conversations (no authentication required)
- Persistent chat history
- LLM-powered responses (OpenRouter)
- Hardcoded FAQ / domain knowledge
- Input validation & error handling
- Clean separation of backend layers

---

## ğŸ—ï¸ Tech Stack

### Frontend

- React.js
- Vite
- Fetch API
- Tailwind CSS

### Backend

- Node.js + TypeScript
- Express.js
- Prisma ORM
- SQLite (local)
- LLM API (OpenRouter)

---

## ğŸ“‚ Repository Structure

```
ai-chatbot/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ prisma/
â”‚ â”‚ â””â”€â”€ schema.prisma
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ server.ts
â”‚ â”‚ â”œâ”€â”€ app.ts
â”‚ â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ controllers/
â”‚ â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ repositories/
â”‚ â”‚ â””â”€â”€ prisma.ts
â”‚ â””â”€â”€ package.json
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”œâ”€â”€ hooks/
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸš€ Running the Project Locally

### .env Setup (backend)

```bash
DATABASE_URL="file:./dev.db"
OPENROUTER_API_KEY=<Openroute Api Key>
# LLM Config
LLM_MODEL=<LLM_MODEL>
# Currently Used mistralai/mistral-7b-instruct:free
```

### Backend Setup

```bash
cd backend
npm install
```

### Database Setup

```bash
npx prisma generate
npx prisma db push
```

### Start Backend

```bash
npm run dev
```

### Backend runs at

```bash
http://localhost:3000
```

### Deployment Notes
The backend is deployed on Renderâ€™s free tier.
Due to free-tier limitations, the service may cold-start after periods of inactivity,
causing the first request to take longer than usual.

### .env Setup (frontend)

```bash
VITE_API_BASE_URL=http://localhost:3000

```

### Frontend Setup

```bash
cd frontend
npm install
```

### Start Frontend

```bash
npm run dev
```

### Frontend runs at

```bash
http://localhost:5173
```

ğŸ”„ Core User Flow

- User opens chat UI
- Types a message
- Frontend validates input (non-empty, character limit)
- Backend:
  - Creates or resumes a session
  - Persists user message
  - Sends conversation context to LLM
  - Saves AI response
- Frontend displays AI reply
- Session ID is reused to load history on refresh

## ğŸ“¡ API Overview

### POST `/chat/message`

**Request**

```json
{
  "message": "What is your return policy?",
  "sessionId": "optional-session-id"
}
```

**Response**
```json
{
"reply": "Our return policy allows returns within 30 days.",
"sessionId": "session-id"
}

```

## ğŸ§  Backend Architecture

- **Routes** â€“ HTTP endpoints
- **Controllers** â€“ Request/response handling
- **Services** â€“ Business logic
- **Repositories** â€“ Database access via Prisma
- **LLM Service** â€“ Encapsulates all AI logic

This structure allows:

- Adding new channels (WhatsApp, Instagram)
- Swapping LLM providers
- Adding tools/function calling in the future

---

## ğŸ¤– LLM Integration

- Provider: OpenRouter
- Prompt includes:
  - System instructions (â€œYou are a helpful support agentâ€¦â€)
  - Recent conversation history
- Max message length enforced to control cost
- All LLM errors are caught and returned as friendly messages

---

## ğŸ›¡ï¸ Robustness & Validation

- Empty messages are rejected
- Character limit enforced on frontend and backend
- Disabled send button during loading
- Graceful handling of:
  - LLM failures
  - Network errors
  - Invalid input
- No hard-coded secrets

---

## ğŸ¨ UX Decisions

- Auto-scroll to latest message
- Clear distinction between user and AI messages
- Character counter with visual warning near limit
- Disabled send button when input is invalid
- Friendly error messages displayed in chat
